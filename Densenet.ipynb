{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1+a4fc05a'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook\n",
    "import util\n",
    "filelist = [f for f in listdir('D:/DataSet/AerialImageDataset/train/images') if isfile(join('D:/DataSet/AerialImageDataset/train/images', f))]\n",
    "labels_path = 'D:/DataSet/AerialImageDataset/train/gt/'\n",
    "images_path = 'D:/DataSet/AerialImageDataset/train/images/'\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "densenet = models.densenet121(pretrained=True)\n",
    "densenet\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dense_Net_Feature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dense_Net_Feature, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layer0 = nn.Sequential(densenet.features.conv0, densenet.features.norm0, densenet.features.relu0, densenet.features.pool0)\n",
    "        self.denseblock1 = densenet.features.denseblock1\n",
    "        self.transition1 = densenet.features.transition1\n",
    "        self.denseblock2 = densenet.features.denseblock2\n",
    "        self.transition2 = densenet.features.transition2\n",
    "        self.denseblock3 = densenet.features.denseblock3\n",
    "        self.transition3 = densenet.features.transition3\n",
    "        self.denseblock4 = densenet.features.denseblock4\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(43, 20, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.1),\n",
    "            nn.Conv2d(20, 2, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.reducedim0 = nn.Conv2d(256,10,kernel_size=3,padding=1,bias=False) \n",
    "        self.reducedim1 = nn.Conv2d(512,10,kernel_size=3,padding=1,bias=False) \n",
    "        self.reducedim2 = nn.Conv2d(1024,10,kernel_size=3,padding=1,bias=False) \n",
    "        self.reducedim3 = nn.Conv2d(1024,10,kernel_size=3,padding=1,bias=False) \n",
    "#         self.ppm = _PyramidPoolingModule(512, 512, (1, 2, 3, 6))\n",
    "#         self.final = nn.Sequential(\n",
    "#             nn.Conv2d(2560, 512, kernel_size=3, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(512, momentum=.95),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "#         )\n",
    "\n",
    "#         if use_aux:\n",
    "#             self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n",
    "#             initialize_weights(self.aux_logits)\n",
    "\n",
    "#         initialize_weights(self.ppm, self.final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        \n",
    "        out = self.layer0(x)\n",
    "        \n",
    "        \n",
    "        out = self.denseblock1(out)\n",
    "        upsample0 = F.upsample(self.reducedim0(out),x_size[2:],mode='bilinear')\n",
    "        out = self.transition1(out)\n",
    "        \n",
    "\n",
    "        out = self.denseblock2(out)\n",
    "        upsample1 = F.upsample(self.reducedim1(out),x_size[2:],mode='bilinear')\n",
    "        out = self.transition2(out)\n",
    "        \n",
    "        \n",
    "        out = self.denseblock3(out)\n",
    "        upsample2 = F.upsample(self.reducedim2(out),x_size[2:],mode='bilinear')\n",
    "        out = self.transition3(out)\n",
    "        out = self.denseblock4(out)\n",
    "        upsample3 = F.upsample(self.reducedim3(out),x_size[2:],mode='bilinear')\n",
    "        \n",
    "        out = self.final(torch.cat((x,upsample0,upsample1,upsample2,upsample3),1))\n",
    "        \n",
    "        return out\n",
    "#         x = self.layer0(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         if self.training and self.use_aux:\n",
    "#             aux = self.aux_logits(x)\n",
    "#         x = self.layer4(x)\n",
    "#         print(x.size())\n",
    "#         x = self.ppm(x)\n",
    "#         x = self.final(x)\n",
    "#         if self.training and self.use_aux:\n",
    "#             return F.upsample(x, x_size[2:], mode='bilinear'), F.upsample(aux, x_size[2:], mode='bilinear')\n",
    "#         return F.upsample(x, x_size[2:], mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "densenet_f = Dense_Net_Feature()\n",
    "CROP_SIZE =500\n",
    "# densenet_f = densenet_f.cuda()\n",
    "# densenet_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if image contain too little one label,250000*5% just skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model ,criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        rand_sample = np.random.randint(len(filelist), size=20)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        \n",
    "            \n",
    "        #scheduler.step()\n",
    "        model.train(True)  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        out_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "#                 normalize\n",
    "        ])\n",
    "        out_transform_lebel = transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "#                 normalize\n",
    "        ])\n",
    "        # Iterate over data.\n",
    "        count = 0\n",
    "        cum = 0\n",
    "        for index in tqdm_notebook(rand_sample,total=20):\n",
    "            im = PIL.Image.open(join(images_path,filelist[index]))\n",
    "            labels_data = PIL.Image.open(join(labels_path,filelist[index]))\n",
    "            \n",
    "            train_data = out_transform(im)\n",
    "            labels_data = out_transform_lebel(labels_data)\n",
    "            im.close()\n",
    "            for i in range(5000//CROP_SIZE):\n",
    "                for j in range(5000//CROP_SIZE):\n",
    "                    \n",
    "                    labels_data_tem = Variable(labels_data[:,i*CROP_SIZE:i*CROP_SIZE+CROP_SIZE,j*CROP_SIZE:j*CROP_SIZE+CROP_SIZE].type(torch.LongTensor).cuda())\n",
    "                    cum = torch.sum(labels_data_tem).cpu().data.numpy()[0]\n",
    "                    if cum < CROP_SIZE*CROP_SIZE*0.04:\n",
    "                        continue\n",
    "                    count+=1\n",
    "                    train_data_tem = train_data[:,i*CROP_SIZE:i*CROP_SIZE+CROP_SIZE,j*CROP_SIZE:j*CROP_SIZE+CROP_SIZE].unsqueeze(0)\n",
    "                    train_data_tem = Variable(train_data_tem.cuda())\n",
    "                    \n",
    "# #                     print(train_data_tem.size())\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "        \n",
    "                    # forward\n",
    "                    outputs= model(train_data_tem)\n",
    "#                     print(labels_data.size())\n",
    "                    loss = criterion(outputs,labels_data_tem) \n",
    "                    \n",
    "                    \n",
    "        \n",
    "                    #loss = MSE(outputs, inputs_eva)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0]\n",
    "        epoch_loss = running_loss / count\n",
    "        if(epoch_loss<best_loss):\n",
    "            best_model_wts = model.state_dict()\n",
    "            best_loss = epoch_loss\n",
    "        print('Loss: {:.4f} Iteration:{}'.format(epoch_loss,count))\n",
    "    # deep copy the model\n",
    "    \n",
    "\n",
    "    print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labels_data = PIL.Image.open(join(labels_path,filelist[0]))\n",
    "# out_transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# # train_data = out_transform(im)\n",
    "# labels_data = out_transform(labels_data)\n",
    "# labels_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# densenet_f.load_state_dict(torch.load('vnew0.pt'))\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "# if use_gpu:\n",
    "#     densenet_f = densenet_f.cuda()\n",
    "criterion = util.CrossEntropyLoss2d(size_average=True).cuda()\n",
    "optimizer_ft = optim.Adam(densenet_f.parameters(), lr=0.000001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "densenet_f = train_model(densenet_f,criterion,optimizer_ft,exp_lr_scheduler,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# torch.save(densenet_f.state_dict(),'vnew5.pt')\n",
    "densenet_f.load_state_dict(torch.load('vnew5.pt'))\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    densenet_f = densenet_f.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_acc/(5*5*len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_im = PIL.Image.open(labels_path+filelist[0])\n",
    "label_im = np.asarray(label_im)\n",
    "label_im = label_im==255\n",
    "\n",
    "label_im2 = PIL.Image.open(labels_path+filelist[2])\n",
    "label_im2 = np.asarray(label_im2)\n",
    "label_im2 = label_im2==255\n",
    "\n",
    "plt.imshow(label_im)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(label_im2)\n",
    "plt.show()\n",
    "(np.logical_and(label_im,label_im2)*1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_path='fuckcity.tif'\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test = test_transform(PIL.Image.open(test_path)).unsqueeze(0)\n",
    "test = Variable(test.cuda(),volatile=True)\n",
    "densenet_f.eval()\n",
    "out = densenet_f(test)\n",
    "m = nn.Softmax2d()\n",
    "out = m(out)\n",
    "out = (out.cpu()[0].data.numpy()).transpose((1, 2, 0))\n",
    "\n",
    "\n",
    "\n",
    "f,ax = plt.subplots(1,2)\n",
    "\n",
    "ax[0].imshow(out[:,:,1])\n",
    "ax[1].imshow(np.array(PIL.Image.open(test_path)))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
